{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_json('../../../data/processed/reviews.json.gz', orient=\"records\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_reviews[['cleaned_review']]\n",
    "y = df_reviews[['sentiment']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of +/- review sentiment: \n",
      "sentiment\n",
      "1            0.883957\n",
      "0            0.116043\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of +/- review sentiment: \\n{}\".format(y_train.value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final = x_train['cleaned_review'].values\n",
    "x_test_final = x_test['cleaned_review'].values\n",
    "y_train_final = y_train['sentiment'].values\n",
    "y_test_final = y_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = {\n",
    "    'tfidf_vectorizer': \n",
    "    {\n",
    "        'model': TfidfVectorizer(ngram_range=(1, 2)), \n",
    "        'search_spaces': \n",
    "        [\n",
    "            {\n",
    "                'vect__min_df': Categorical([3]),\n",
    "                'vect__max_df': Categorical([0.99]),\n",
    "                'vect__sublinear_tf': Categorical([True]),\n",
    "                'vect__max_features': Categorical([12000])\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'count_vectorizer': \n",
    "    {\n",
    "        'model': CountVectorizer(ngram_range=(1, 2)),\n",
    "        'search_spaces': \n",
    "        [\n",
    "            {\n",
    "                'vect__min_df': Categorical([3]),\n",
    "                'vect__max_df': Categorical([0.99]),\n",
    "                'vect__max_features': Categorical([12000])\n",
    "            }     \n",
    "        ]\n",
    "    } \n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    'logistic_regression': \n",
    "    {\n",
    "        'model': LogisticRegression(), \n",
    "        'search_spaces': \n",
    "        [\n",
    "            {\n",
    "                'clf__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "                'clf__penalty': Categorical(['l2']),\n",
    "                'clf__solver': Categorical(['lbfgs', 'newton-cg', 'newton-cholesky', 'sag']),\n",
    "                'clf__max_iter': Integer(low=100, high=1000),\n",
    "            },\n",
    "            {\n",
    "                'clf__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "                'clf__penalty': Categorical(['l1', 'l2']),\n",
    "                'clf__solver': Categorical(['liblinear']),\n",
    "                'clf__max_iter': Integer(low=100, high=1000),\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'linear_svc':\n",
    "    {\n",
    "        'model': LinearSVC(),\n",
    "        'search_spaces':\n",
    "        [\n",
    "            {\n",
    "                'clf__C': Real(1e-6, 1e+6, 'log-uniform'),\n",
    "                'clf__penalty': Categorical(['l2']),\n",
    "                'clf__loss': Categorical(['squared_hinge']),\n",
    "                'clf__dual': Categorical([True, False]),\n",
    "                'clf__tol': Real(1e-6, 1e-2, 'log-uniform'),\n",
    "                'clf__class_weight': Categorical(['balanced']),\n",
    "                'clf__max_iter': Integer(low=100, high=1000),\n",
    "            },\n",
    "            {\n",
    "                'clf__C': Real(1e-6, 1e+6, 'log-uniform'),\n",
    "                'clf__penalty': Categorical(['l1']),\n",
    "                'clf__loss': Categorical(['squared_hinge']),\n",
    "                'clf__dual': Categorical([False]),\n",
    "                'clf__tol': Real(1e-6, 1e-2, 'log-uniform'),\n",
    "                'clf__class_weight': Categorical(['balanced']),\n",
    "                'clf__max_iter': Integer(low=100, high=1000),\n",
    "            },\n",
    "            {\n",
    "                'clf__C': Real(1e-6, 1e+6, 'log-uniform'),\n",
    "                'clf__penalty': Categorical(['l2']),\n",
    "                'clf__loss': Categorical(['hinge']),\n",
    "                'clf__dual': Categorical([True]),\n",
    "                'clf__tol': Real(1e-6, 1e-2, 'log-uniform'),\n",
    "                'clf__class_weight': Categorical(['balanced']),\n",
    "                'clf__max_iter': Integer(low=100, high=1000),\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    'multinomial_nb': \n",
    "    {\n",
    "        'model': MultinomialNB(), \n",
    "        'search_spaces': \n",
    "        [\n",
    "            {\n",
    "                'clf__alpha': Real(low=1e-6, high=1e+6, prior='log-uniform'),\n",
    "                'clf__fit_prior': Categorical([True, False])\n",
    "            }            \n",
    "        ]\n",
    "    },\n",
    "    'decision_tree': \n",
    "    {\n",
    "        'model': DecisionTreeClassifier(), \n",
    "        'search_spaces': \n",
    "        [\n",
    "            {\n",
    "                'clf__criterion': Categorical(['gini', 'entropy']),\n",
    "                'clf__max_depth': Integer(5, 50),\n",
    "                'clf__min_samples_split': Integer(2, 20),\n",
    "                'clf__min_samples_leaf': Integer(1, 10)\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'random_forest': \n",
    "    {\n",
    "        'model': RandomForestClassifier(), \n",
    "        'search_spaces': \n",
    "        [\n",
    "            {\n",
    "                'clf__n_estimators': Integer(50, 500),\n",
    "                'clf__max_depth': Integer(5, 50),\n",
    "                'clf__max_features': Integer(5, 50),\n",
    "                'clf__min_samples_split': Integer(2, 20),\n",
    "                'clf__min_samples_leaf': Integer(1, 10)\n",
    "            }            \n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(vectorizers = vectorizers, classifiers = classifiers):\n",
    "    pipelines = {}\n",
    "\n",
    "    for c_key in classifiers:\n",
    "        for v_key in vectorizers:\n",
    "            pipeline = Pipeline([\n",
    "                ('vect', vectorizers[v_key]['model']), \n",
    "                ('clf', classifiers[c_key]['model'])\n",
    "            ])\n",
    "            \n",
    "            search_spaces = []\n",
    "            for v_item in vectorizers[v_key]['search_spaces']:\n",
    "                for e_item in classifiers[c_key]['search_spaces']:\n",
    "                    search_spaces.append(v_item | e_item)\n",
    "                    \n",
    "            pipelines[f'{c_key}_with_{v_key}'] = {\n",
    "                'pipeline': pipeline,\n",
    "                'search_spaces': search_spaces\n",
    "            }\n",
    "\n",
    "    return pipelines\n",
    "\n",
    "def comparison_table(weighted_avg_f1_score):\n",
    "    df_model = pd.DataFrame(index=weighted_avg_f1_score.keys(), columns=['weighted_avg_f1_score'])\n",
    "    df_model['weighted_avg_f1_score'] = weighted_avg_f1_score.values()\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = create_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Model [logistic_regression_with_tfidf_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', LogisticRegression())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='identity'), 'clf__penalty': Categorical(categories=('l2',), prior=None), 'clf__solver': Categorical(categories=('lbfgs', 'newton-cg', 'newton-cholesky', 'sag'), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='identity')}, {'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='identity'), 'clf__penalty': Categorical(categories=('l1', 'l2'), prior=None), 'clf__solver': Categorical(categories=('liblinear',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='identity')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__C', 3.317697704417197), ('clf__max_iter', 928), ('clf__penalty', 'l2'), ('clf__solver', 'sag'), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3), ('vect__sublinear_tf', True)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2), sublinear_tf=True)),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=3.317697704417197, max_iter=928,\n",
      "                                    solver='sag'))])\n",
      "Best score: 0.934533519911913\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.934533519911913\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.9347273678607169\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.62      0.70      1365\n",
      "           1       0.95      0.98      0.97     10547\n",
      "\n",
      "    accuracy                           0.94     11912\n",
      "   macro avg       0.88      0.80      0.83     11912\n",
      "weighted avg       0.93      0.94      0.93     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [logistic_regression_with_count_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', LogisticRegression())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'), 'clf__penalty': Categorical(categories=('l2',), prior=None), 'clf__solver': Categorical(categories=('lbfgs', 'newton-cg', 'newton-cholesky', 'sag'), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='normalize')}, {'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'), 'clf__penalty': Categorical(categories=('l1', 'l2'), prior=None), 'clf__solver': Categorical(categories=('liblinear',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='normalize')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__C', 0.21776603694820984), ('clf__max_iter', 927), ('clf__penalty', 'l2'), ('clf__solver', 'newton-cg'), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=0.21776603694820984, max_iter=927,\n",
      "                                    solver='newton-cg'))])\n",
      "Best score: 0.9313217667417897\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.9313217667417897\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.930987278181686\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.63      0.68      1365\n",
      "           1       0.95      0.97      0.96     10547\n",
      "\n",
      "    accuracy                           0.93     11912\n",
      "   macro avg       0.85      0.80      0.82     11912\n",
      "weighted avg       0.93      0.93      0.93     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [linear_svc_with_tfidf_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', LinearSVC())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='identity'), 'clf__penalty': Categorical(categories=('l2',), prior=None), 'clf__loss': Categorical(categories=('squared_hinge',), prior=None), 'clf__dual': Categorical(categories=(True, False), prior=None), 'clf__tol': Real(low=1e-06, high=0.01, prior='log-uniform', transform='identity'), 'clf__class_weight': Categorical(categories=('balanced',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='identity')}, {'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='identity'), 'clf__penalty': Categorical(categories=('l1',), prior=None), 'clf__loss': Categorical(categories=('squared_hinge',), prior=None), 'clf__dual': Categorical(categories=(False,), prior=None), 'clf__tol': Real(low=1e-06, high=0.01, prior='log-uniform', transform='identity'), 'clf__class_weight': Categorical(categories=('balanced',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='identity')}, {'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='identity'), 'clf__penalty': Categorical(categories=('l2',), prior=None), 'clf__loss': Categorical(categories=('hinge',), prior=None), 'clf__dual': Categorical(categories=(True,), prior=None), 'clf__tol': Real(low=1e-06, high=0.01, prior='log-uniform', transform='identity'), 'clf__class_weight': Categorical(categories=('balanced',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='identity')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__C', 0.21776603694820984), ('clf__class_weight', 'balanced'), ('clf__dual', False), ('clf__loss', 'squared_hinge'), ('clf__max_iter', 269), ('clf__penalty', 'l2'), ('clf__tol', 4.185932455010554e-06), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3), ('vect__sublinear_tf', True)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2), sublinear_tf=True)),\n",
      "                ('clf',\n",
      "                 LinearSVC(C=0.21776603694820984, class_weight='balanced',\n",
      "                           dual=False, max_iter=269,\n",
      "                           tol=4.185932455010554e-06))])\n",
      "Best score: 0.9208782880297983\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.9208782880297983\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.9173473844007881\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.85      0.68      1365\n",
      "           1       0.98      0.92      0.95     10547\n",
      "\n",
      "    accuracy                           0.91     11912\n",
      "   macro avg       0.78      0.88      0.82     11912\n",
      "weighted avg       0.93      0.91      0.92     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [linear_svc_with_count_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', LinearSVC())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'), 'clf__penalty': Categorical(categories=('l2',), prior=None), 'clf__loss': Categorical(categories=('squared_hinge',), prior=None), 'clf__dual': Categorical(categories=(True, False), prior=None), 'clf__tol': Real(low=1e-06, high=0.01, prior='log-uniform', transform='normalize'), 'clf__class_weight': Categorical(categories=('balanced',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='normalize')}, {'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'), 'clf__penalty': Categorical(categories=('l1',), prior=None), 'clf__loss': Categorical(categories=('squared_hinge',), prior=None), 'clf__dual': Categorical(categories=(False,), prior=None), 'clf__tol': Real(low=1e-06, high=0.01, prior='log-uniform', transform='normalize'), 'clf__class_weight': Categorical(categories=('balanced',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='normalize')}, {'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'), 'clf__penalty': Categorical(categories=('l2',), prior=None), 'clf__loss': Categorical(categories=('hinge',), prior=None), 'clf__dual': Categorical(categories=(True,), prior=None), 'clf__tol': Real(low=1e-06, high=0.01, prior='log-uniform', transform='normalize'), 'clf__class_weight': Categorical(categories=('balanced',), prior=None), 'clf__max_iter': Integer(low=100, high=1000, prior='uniform', transform='normalize')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__C', 0.21776603694820984), ('clf__class_weight', 'balanced'), ('clf__dual', False), ('clf__loss', 'squared_hinge'), ('clf__max_iter', 269), ('clf__penalty', 'l2'), ('clf__tol', 4.185932455010554e-06), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('clf',\n",
      "                 LinearSVC(C=0.21776603694820984, class_weight='balanced',\n",
      "                           dual=False, max_iter=269,\n",
      "                           tol=4.185932455010554e-06))])\n",
      "Best score: 0.919950414060635\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.919950414060635\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.9170205467161967\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.72      0.66      1365\n",
      "           1       0.96      0.94      0.95     10547\n",
      "\n",
      "    accuracy                           0.91     11912\n",
      "   macro avg       0.78      0.83      0.80     11912\n",
      "weighted avg       0.92      0.91      0.92     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [multinomial_nb_with_tfidf_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', MultinomialNB())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__alpha': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='identity'), 'clf__fit_prior': Categorical(categories=(True, False), prior=None)}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__alpha', 0.08341564384216595), ('clf__fit_prior', True), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3), ('vect__sublinear_tf', True)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2), sublinear_tf=True)),\n",
      "                ('clf', MultinomialNB(alpha=0.08341564384216595))])\n",
      "Best score: 0.9193322139833899\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.9193322139833899\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.9224916112579281\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.51      0.63      1365\n",
      "           1       0.94      0.98      0.96     10547\n",
      "\n",
      "    accuracy                           0.93     11912\n",
      "   macro avg       0.87      0.75      0.79     11912\n",
      "weighted avg       0.92      0.93      0.92     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [multinomial_nb_with_count_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', MultinomialNB())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__alpha': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'), 'clf__fit_prior': Categorical(categories=(True, False), prior=None)}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__alpha', 1.10551257524209e-06), ('clf__fit_prior', True), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('clf', MultinomialNB(alpha=1.10551257524209e-06))])\n",
      "Best score: 0.9211128739121875\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.9211128739121875\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.9177835576726017\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.66      1365\n",
      "           1       0.96      0.94      0.95     10547\n",
      "\n",
      "    accuracy                           0.91     11912\n",
      "   macro avg       0.78      0.83      0.81     11912\n",
      "weighted avg       0.92      0.91      0.92     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [decision_tree_with_tfidf_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', DecisionTreeClassifier())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__criterion': Categorical(categories=('gini', 'entropy'), prior=None), 'clf__max_depth': Integer(low=5, high=50, prior='uniform', transform='identity'), 'clf__min_samples_split': Integer(low=2, high=20, prior='uniform', transform='identity'), 'clf__min_samples_leaf': Integer(low=1, high=10, prior='uniform', transform='identity')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__criterion', 'gini'), ('clf__max_depth', 13), ('clf__min_samples_leaf', 6), ('clf__min_samples_split', 16), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3), ('vect__sublinear_tf', True)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2), sublinear_tf=True)),\n",
      "                ('clf',\n",
      "                 DecisionTreeClassifier(max_depth=13, min_samples_leaf=6,\n",
      "                                        min_samples_split=16))])\n",
      "Best score: 0.8978290544328736\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.8978290544328736\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.8951965441911282\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.37      0.48      1365\n",
      "           1       0.92      0.98      0.95     10547\n",
      "\n",
      "    accuracy                           0.91     11912\n",
      "   macro avg       0.80      0.67      0.71     11912\n",
      "weighted avg       0.89      0.91      0.90     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [decision_tree_with_count_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', DecisionTreeClassifier())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__criterion': Categorical(categories=('gini', 'entropy'), prior=None), 'clf__max_depth': Integer(low=5, high=50, prior='uniform', transform='normalize'), 'clf__min_samples_split': Integer(low=2, high=20, prior='uniform', transform='normalize'), 'clf__min_samples_leaf': Integer(low=1, high=10, prior='uniform', transform='normalize')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__criterion', 'gini'), ('clf__max_depth', 37), ('clf__min_samples_leaf', 9), ('clf__min_samples_split', 9), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('clf',\n",
      "                 DecisionTreeClassifier(max_depth=37, min_samples_leaf=9,\n",
      "                                        min_samples_split=9))])\n",
      "Best score: 0.8992561023382729\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.8992561023382729\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.900375166671821\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.47      0.53      1365\n",
      "           1       0.93      0.96      0.95     10547\n",
      "\n",
      "    accuracy                           0.91     11912\n",
      "   macro avg       0.78      0.72      0.74     11912\n",
      "weighted avg       0.90      0.91      0.90     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [random_forest_with_tfidf_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', RandomForestClassifier())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__sublinear_tf': Categorical(categories=(True,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__n_estimators': Integer(low=50, high=500, prior='uniform', transform='identity'), 'clf__max_depth': Integer(low=5, high=50, prior='uniform', transform='identity'), 'clf__max_features': Integer(low=5, high=50, prior='uniform', transform='identity'), 'clf__min_samples_split': Integer(low=2, high=20, prior='uniform', transform='identity'), 'clf__min_samples_leaf': Integer(low=1, high=10, prior='uniform', transform='identity')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__max_depth', 38), ('clf__max_features', 47), ('clf__min_samples_leaf', 2), ('clf__min_samples_split', 5), ('clf__n_estimators', 410), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3), ('vect__sublinear_tf', True)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2), sublinear_tf=True)),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(max_depth=38, max_features=47,\n",
      "                                        min_samples_leaf=2, min_samples_split=5,\n",
      "                                        n_estimators=410))])\n",
      "Best score: 0.830875540749709\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.830875540749709\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.8324145235494719\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.01      1365\n",
      "           1       0.89      1.00      0.94     10547\n",
      "\n",
      "    accuracy                           0.89     11912\n",
      "   macro avg       0.94      0.50      0.47     11912\n",
      "weighted avg       0.90      0.89      0.83     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Train Model [random_forest_with_count_vectorizer]:\n",
      "Pipeline:\n",
      "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('clf', RandomForestClassifier())])\n",
      "Search Spaces:\n",
      "[{'vect__min_df': Categorical(categories=(3,), prior=None), 'vect__max_df': Categorical(categories=(0.99,), prior=None), 'vect__max_features': Categorical(categories=(12000,), prior=None), 'clf__n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'), 'clf__max_depth': Integer(low=5, high=50, prior='uniform', transform='normalize'), 'clf__max_features': Integer(low=5, high=50, prior='uniform', transform='normalize'), 'clf__min_samples_split': Integer(low=2, high=20, prior='uniform', transform='normalize'), 'clf__min_samples_leaf': Integer(low=1, high=10, prior='uniform', transform='normalize')}]\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best hyperparameters: OrderedDict([('clf__max_depth', 38), ('clf__max_features', 47), ('clf__min_samples_leaf', 2), ('clf__min_samples_split', 5), ('clf__n_estimators', 410), ('vect__max_df', 0.99), ('vect__max_features', 12000), ('vect__min_df', 3)])\n",
      "Best estimator: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.99, max_features=12000, min_df=3,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(max_depth=38, max_features=47,\n",
      "                                        min_samples_leaf=2, min_samples_split=5,\n",
      "                                        n_estimators=410))])\n",
      "Best score: 0.8317976988176052\n",
      "\n",
      "Features extracted:\n",
      "Features: ['aaa' 'aaa battery' 'ability' ... 'zoom take' 'zoom work' 'zotac']\n",
      "Length: 12000\n",
      "\n",
      "Evaluation using cross validation (validation set):\n",
      "weighted average f1 score: 0.8317976988176052\n",
      "\n",
      "Evaluation using hold-out validation (test set):\n",
      "weighted average f1 score: 0.8320061242645738\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1365\n",
      "           1       0.89      1.00      0.94     10547\n",
      "\n",
      "    accuracy                           0.89     11912\n",
      "   macro avg       0.94      0.50      0.47     11912\n",
      "weighted avg       0.90      0.89      0.83     11912\n",
      "\n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_weighted_score_val_dict = {}\n",
    "f1_weighted_score_test_dict = {}\n",
    "\n",
    "cv=StratifiedKFold(n_splits=5)\n",
    "\n",
    "for key in pipelines:\n",
    "    pipeline = pipelines[key]['pipeline']\n",
    "    search_spaces = pipelines[key]['search_spaces']\n",
    "    \n",
    "    search = BayesSearchCV(\n",
    "                estimator=pipeline, \n",
    "                search_spaces=search_spaces, \n",
    "                scoring='f1_weighted',\n",
    "                cv=cv, n_iter=10, n_jobs=-1, verbose=3, random_state=42\n",
    "            )\n",
    "    \n",
    "    print(f'Train Model [{key}]:')\n",
    "    print(f'Pipeline:\\n{pipeline}')\n",
    "    print(f'Search Spaces:\\n{search_spaces}')\n",
    "    search.fit(x_train_final, y_train_final)\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best estimator: {search.best_estimator_}\")\n",
    "    print(f\"Best score: {search.best_score_}\")\n",
    "\n",
    "    print(f'\\nFeatures extracted:')\n",
    "    feature_names = search.best_estimator_.named_steps['vect'].get_feature_names_out()\n",
    "    print(f'Features: {feature_names}')\n",
    "    print(f'Length: {len(feature_names)}')\n",
    "\n",
    "    f1_weighted_score_val = search.best_score_\n",
    "    f1_weighted_score_val_dict[key] = f1_weighted_score_val\n",
    "    print(f'\\nEvaluation using cross validation (validation set):')\n",
    "    print(f'weighted average f1 score: {f1_weighted_score_val}')\n",
    "    \n",
    "    y_pred = search.predict(x_test_final)\n",
    "    f1_weighted_score_test = f1_score(y_test_final, y_pred, average='weighted')\n",
    "    f1_weighted_score_test_dict[key] = f1_weighted_score_test\n",
    "    \n",
    "    print(f'\\nEvaluation using hold-out validation (test set):')\n",
    "    print(f'weighted average f1 score: {f1_weighted_score_test}')\n",
    "    \n",
    "    print('\\nClassification report:')\n",
    "    print(classification_report(y_test_final, y_pred, labels=[0, 1]))\n",
    "    \n",
    "    pickle.dump(\n",
    "        search.best_estimator_, \n",
    "        open(f'../../../models/sentiment_analysis/hyperparameter_tuning/{key}.pkl', 'wb')\n",
    "    )\n",
    "    \n",
    "    print(f'------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric for Different Models Using Validation Set:\n",
      "                                           weighted_avg_f1_score\n",
      "logistic_regression_with_tfidf_vectorizer               0.934534\n",
      "logistic_regression_with_count_vectorizer               0.931322\n",
      "linear_svc_with_tfidf_vectorizer                        0.920878\n",
      "linear_svc_with_count_vectorizer                        0.919950\n",
      "multinomial_nb_with_tfidf_vectorizer                    0.919332\n",
      "multinomial_nb_with_count_vectorizer                    0.921113\n",
      "decision_tree_with_tfidf_vectorizer                     0.897829\n",
      "decision_tree_with_count_vectorizer                     0.899256\n",
      "random_forest_with_tfidf_vectorizer                     0.830876\n",
      "random_forest_with_count_vectorizer                     0.831798\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation Metric for Different Models Using Validation Set:')\n",
    "print(comparison_table(f1_weighted_score_val_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric for Different Models Using Testing Set:\n",
      "                                           weighted_avg_f1_score\n",
      "logistic_regression_with_tfidf_vectorizer               0.934727\n",
      "logistic_regression_with_count_vectorizer               0.930987\n",
      "linear_svc_with_tfidf_vectorizer                        0.917347\n",
      "linear_svc_with_count_vectorizer                        0.917021\n",
      "multinomial_nb_with_tfidf_vectorizer                    0.922492\n",
      "multinomial_nb_with_count_vectorizer                    0.917784\n",
      "decision_tree_with_tfidf_vectorizer                     0.895197\n",
      "decision_tree_with_count_vectorizer                     0.900375\n",
      "random_forest_with_tfidf_vectorizer                     0.832415\n",
      "random_forest_with_count_vectorizer                     0.832006\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation Metric for Different Models Using Testing Set:')\n",
    "print(comparison_table(f1_weighted_score_test_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
