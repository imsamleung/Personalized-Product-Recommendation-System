{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_load_model_path = '../../../models/sentiment_analysis/hyperparameter_tuning'\n",
    "root_load_model_path = '../../../models/sentiment_analysis/hyperparameter_tuning_and_oversampling'\n",
    "# root_load_model_path = '../../../models/sentiment_analysis/baseline'\n",
    "# root_load_model_path = '../../../models/sentiment_analysis/oversampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [\n",
    "    'tfidf_vectorizer', \n",
    "    'count_vectorizer'\n",
    "]\n",
    "classifiers = [\n",
    "    'logistic_regression', \n",
    "    'linear_svc', \n",
    "    'multinomial_nb', \n",
    "    'decision_tree', \n",
    "    'random_forest'\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for c in classifiers:\n",
    "    for v in vectorizers:\n",
    "        name = f'{c}_with_{v}'\n",
    "        models[name] = pickle.load(open(f'{root_load_model_path}/{name}.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_json('../../../data/processed/reviews.json.gz', orient=\"records\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_reviews[['cleaned_review']]\n",
    "y = df_reviews[['sentiment']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "x_test_final = x_test['cleaned_review'].values\n",
    "y_test_final = y_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sensitivity(cm):\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "    return round(TP/float(FN + TP), 2)\n",
    "\n",
    "def cal_specificity(cm):\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    return round(TN / float(TN + FP), 2)\n",
    "\n",
    "def comparison_table(f1_weighted_score_dict):\n",
    "    df_model = pd.DataFrame(index=f1_weighted_score_dict.keys(), columns=['f1_weighted_score'])\n",
    "    df_model['f1_weighted_score'] = f1_weighted_score_dict.values()\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted_score_test_dict = {}\n",
    "\n",
    "for key in models:\n",
    "   model = models[key]\n",
    "\n",
    "   print(f\"Evaluate Model [{key}]:\")\n",
    "   print(model)\n",
    "\n",
    "   y_pred = model.predict(x_test_final)\n",
    "   f1_weighted_score_test = f1_score(y_test_final, y_pred, average = 'weighted')\n",
    "   f1_weighted_score_test_dict[key] = f1_weighted_score_test\n",
    "   \n",
    "   print(f'\\nEvaluation using hold-out validation (test set):')\n",
    "   print(f'weighted average f1 score: {f1_weighted_score_test}')\n",
    "   \n",
    "   print(\"\\nClassification Report:\")\n",
    "   print(classification_report(y_test_final, y_pred, labels=[0, 1]))\n",
    "\n",
    "   print(\"\\nConfusion Matrix:\")\n",
    "   cm = confusion_matrix(y_test_final, y_pred, labels=[0, 1])\n",
    "   cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "   fig, ax = plt.subplots(figsize=(4,4))\n",
    "   cm_display.plot(ax=ax)\n",
    "   plt.show()\n",
    "   print(\"sensitivity (true positive): {}\".format(cal_sensitivity(cm)))\n",
    "   print(\"specificity (true negative): {}\".format(cal_specificity(cm)))\n",
    "   \n",
    "   print('----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           weighted_avg_f1_score\n",
      "logistic_regression_with_tfidf_vectorizer               0.888771\n",
      "logistic_regression_with_count_vectorizer               0.888222\n",
      "linear_svc_with_tfidf_vectorizer                        0.890962\n",
      "linear_svc_with_count_vectorizer                        0.892553\n",
      "multinomial_nb_with_tfidf_vectorizer                    0.880817\n",
      "multinomial_nb_with_count_vectorizer                    0.892577\n",
      "decision_tree_with_tfidf_vectorizer                     0.846673\n",
      "decision_tree_with_count_vectorizer                     0.862782\n",
      "random_forest_with_tfidf_vectorizer                     0.915327\n",
      "random_forest_with_count_vectorizer                     0.891617\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation Metric for Different Models Using Testing Set:')\n",
    "print(comparison_table(f1_weighted_score_test_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
